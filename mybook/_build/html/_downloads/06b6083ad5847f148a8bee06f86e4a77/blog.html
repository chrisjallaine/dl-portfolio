<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimizer Showdown: Theory Meets Practice in Deep Learning</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600;700&family=Inter:wght@300;400;500;600&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --bg-light: #fdf6f0;
            --bg-light-alt: #fff5f5;
            --bg-light-tertiary: #fef0f0;
            --text-primary: #2c1810;
            --text-secondary: #3d2817;
            --text-tertiary: #5a3d2b;
            --accent-primary: #8B4513;
            --accent-secondary: #DC143C;
            --accent-light: #a67c52;
            --border-color: rgba(139, 69, 19, 0.1);
            --card-bg: rgba(255, 255, 255, 0.7);
            --shadow-sm: 0 10px 30px rgba(139, 69, 19, 0.08);
            --shadow-md: 0 15px 50px rgba(139, 69, 19, 0.1);
            --shadow-lg: 0 20px 60px rgba(139, 69, 19, 0.15);
        }

        body.dark-mode {
            --bg-light: #1a1410;
            --bg-light-alt: #241a14;
            --bg-light-tertiary: #2a1f18;
            --text-primary: #f5e6dd;
            --text-secondary: #e8d5c8;
            --text-tertiary: #d4bfb0;
            --card-bg: rgba(40, 30, 25, 0.7);
            --border-color: rgba(220, 20, 60, 0.15);
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', sans-serif;
            background: linear-gradient(135deg, var(--bg-light) 0%, var(--bg-light-alt) 50%, var(--bg-light-tertiary) 100%);
            color: var(--text-primary);
            line-height: 1.8;
            font-size: 18px;
            overflow-x: hidden;
            transition: background 0.3s ease, color 0.3s ease;
        }

        .floating-shapes {
            position: fixed;
            width: 100%;
            height: 100%;
            top: 0;
            left: 0;
            z-index: 0;
            pointer-events: none;
            overflow: hidden;
        }

        .shape {
            position: absolute;
            border-radius: 50%;
            opacity: 0.03;
            animation: float 20s infinite ease-in-out;
            transition: opacity 0.3s ease;
        }

        body.dark-mode .shape {
            opacity: 0.05;
        }

        .shape1 {
            width: 400px;
            height: 400px;
            background: var(--accent-primary);
            top: 10%;
            left: -10%;
            animation-delay: 0s;
        }

        .shape2 {
            width: 300px;
            height: 300px;
            background: var(--accent-secondary);
            bottom: 20%;
            right: -5%;
            animation-delay: 5s;
        }

        .shape3 {
            width: 250px;
            height: 250px;
            background: #8B0000;
            top: 50%;
            right: 10%;
            animation-delay: 10s;
        }

        @keyframes float {
            0%, 100% { transform: translate(0, 0) scale(1); }
            33% { transform: translate(30px, -50px) scale(1.1); }
            66% { transform: translate(-20px, 30px) scale(0.9); }
        }

        .theme-toggle {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 999;
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 50px;
            padding: 10px 15px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            backdrop-filter: blur(10px);
        }

        .theme-toggle:hover {
            transform: scale(1.1);
            box-shadow: var(--shadow-sm);
        }

        .header {
            position: relative;
            background: linear-gradient(135deg, rgba(139, 69, 19, 0.05) 0%, rgba(220, 20, 60, 0.05) 100%);
            padding: 120px 40px 80px 40px;
            text-align: center;
            border-bottom: 2px solid var(--border-color);
            z-index: 1;
            transition: background 0.3s ease;
        }

        .header h1 {
            font-family: 'Playfair Display', serif;
            font-size: 4em;
            background: linear-gradient(135deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 30px;
            font-weight: 700;
            letter-spacing: -1px;
            line-height: 1.2;
            animation: fadeInUp 1s ease;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .header .subtitle {
            font-size: 1.3em;
            color: var(--accent-primary);
            font-weight: 300;
            margin-bottom: 30px;
            opacity: 0.8;
            animation: fadeInUp 1s ease 0.2s backwards;
        }

        .header .meta {
            color: var(--accent-light);
            font-size: 0.9em;
            letter-spacing: 1px;
            font-weight: 500;
            animation: fadeInUp 1s ease 0.4s backwards;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 80px 60px;
            position: relative;
            z-index: 1;
        }

        .content-section {
            margin: 80px 0;
            animation: fadeIn 1s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        h2 {
            font-family: 'Playfair Display', serif;
            font-size: 2.8em;
            background: linear-gradient(135deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin: 100px 0 50px 0;
            font-weight: 600;
            letter-spacing: -0.5px;
            position: relative;
            padding-bottom: 20px;
        }

        h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 80px;
            height: 3px;
            background: linear-gradient(90deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            border-radius: 3px;
        }

        h3 {
            font-family: 'Playfair Display', serif;
            font-size: 2em;
            color: var(--accent-primary);
            margin: 60px 0 30px 0;
            font-weight: 600;
        }

        p {
            margin: 35px 0;
            text-align: justify;
            color: var(--text-secondary);
            transition: color 0.3s ease;
        }

        .lead {
            font-size: 1.3em;
            line-height: 1.9;
            color: var(--text-tertiary);
            margin: 50px 0;
            padding: 50px;
            background: var(--card-bg);
            border-left: 4px solid var(--accent-primary);
            border-radius: 0 15px 15px 0;
            box-shadow: var(--shadow-md);
            transition: all 0.3s ease;
            letter-spacing: 0.3px;
        }

        .lead:hover {
            transform: translateX(5px);
            box-shadow: 0 15px 40px rgba(139, 69, 19, 0.15);
        }

        .formula-block {
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 15px;
            padding: 35px;
            margin: 50px 0;
            font-family: 'Courier New', monospace;
            color: var(--text-tertiary);
            font-size: 0.95em;
            line-height: 1.9;
            overflow-x: auto;
            box-shadow: var(--shadow-md);
            transition: all 0.3s ease;
            position: relative;
        }

        .formula-block::before {
            content: '‚àá';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 3em;
            color: rgba(139, 69, 19, 0.1);
            font-family: serif;
        }

        .formula-block:hover {
            transform: translateY(-3px);
            box-shadow: 0 15px 40px rgba(139, 69, 19, 0.12);
        }

        .code-block {
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 15px;
            padding: 35px;
            margin: 50px 0;
            font-family: 'Courier New', monospace;
            color: var(--text-tertiary);
            font-size: 0.85em;
            line-height: 1.6;
            overflow-x: auto;
            box-shadow: var(--shadow-md);
            transition: all 0.3s ease;
        }

        .code-block:hover {
            transform: translateY(-3px);
            box-shadow: 0 15px 40px rgba(139, 69, 19, 0.12);
        }

        .highlight {
            background: linear-gradient(135deg, rgba(220, 20, 60, 0.05) 0%, rgba(139, 69, 19, 0.05) 100%);
            border-left: 4px solid var(--accent-secondary);
            padding: 35px;
            margin: 50px 0;
            border-radius: 0 15px 15px 0;
            box-shadow: var(--shadow-md);
            transition: transform 0.3s ease;
        }

        .highlight:hover {
            transform: translateX(5px);
        }

        .experiment-card {
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 20px;
            padding: 50px;
            margin: 50px 0;
            box-shadow: var(--shadow-md);
            transition: all 0.4s ease;
            position: relative;
            overflow: hidden;
            opacity: 1;
            transform: translateY(0);
        }

        .experiment-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, rgba(139, 69, 19, 0.03) 0%, rgba(220, 20, 60, 0.03) 100%);
            opacity: 0;
            transition: opacity 0.4s ease;
            pointer-events: none;
        }

        .experiment-card:hover::before {
            opacity: 1;
        }

        .experiment-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 25px 70px rgba(139, 69, 19, 0.15);
            border-color: rgba(139, 69, 19, 0.2);
        }

        .results-table {
            width: 100%;
            margin: 40px 0;
            border-collapse: collapse;
            background: var(--card-bg);
            border-radius: 15px;
            overflow: hidden;
            box-shadow: var(--shadow-md);
        }

        .results-table thead {
            background: linear-gradient(135deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            color: white;
        }

        .results-table th,
        .results-table td {
            padding: 15px 20px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        .results-table tbody tr:hover {
            background: rgba(139, 69, 19, 0.05);
        }

        .results-table tbody tr:last-child td {
            border-bottom: none;
        }

        .chart-container {
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 20px;
            padding: 40px;
            margin: 50px 0;
            box-shadow: var(--shadow-md);
            transition: all 0.3s ease;
        }

        .chart-container:hover {
            transform: translateY(-5px);
            box-shadow: 0 20px 50px rgba(139, 69, 19, 0.15);
        }

        .divider {
            width: 100px;
            height: 3px;
            background: linear-gradient(90deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            margin: 80px auto;
            border-radius: 3px;
            position: relative;
            animation: fadeIn 1s ease;
        }

        .divider::before,
        .divider::after {
            content: '';
            position: absolute;
            width: 8px;
            height: 8px;
            background: var(--accent-secondary);
            border-radius: 50%;
            top: -2.5px;
        }

        .divider::before {
            left: -15px;
        }

        .divider::after {
            right: -15px;
        }

        .tag-container {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 30px 0;
            justify-content: center;
        }

        .tag {
            padding: 8px 20px;
            background: rgba(139, 69, 19, 0.1);
            border: 1px solid rgba(139, 69, 19, 0.2);
            border-radius: 25px;
            font-size: 0.85em;
            color: var(--accent-primary);
            transition: all 0.3s ease;
            cursor: default;
        }

        .tag:hover {
            background: rgba(139, 69, 19, 0.15);
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(139, 69, 19, 0.2);
        }

        .scroll-progress {
            position: fixed;
            top: 0;
            left: 0;
            width: 0%;
            height: 4px;
            background: linear-gradient(90deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            z-index: 1000;
            transition: width 0.1s ease;
            box-shadow: 0 0 10px rgba(220, 20, 60, 0.5);
        }

        .back-to-top {
            position: fixed;
            bottom: 40px;
            right: 40px;
            width: 60px;
            height: 60px;
            background: linear-gradient(135deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 24px;
            cursor: pointer;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            z-index: 1000;
            box-shadow: 0 10px 30px rgba(139, 69, 19, 0.3);
            border: none;
        }

        .back-to-top.visible {
            opacity: 1;
            visibility: visible;
        }

        .back-to-top:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 40px rgba(139, 69, 19, 0.4);
        }

        blockquote {
            border-left: 4px solid var(--accent-primary);
            padding: 30px 30px 30px 40px;
            margin: 60px 0;
            font-style: italic;
            color: var(--text-tertiary);
            font-size: 1.2em;
            background: var(--card-bg);
            border-radius: 0 15px 15px 0;
            box-shadow: var(--shadow-md);
            position: relative;
            transition: all 0.3s ease;
        }

        blockquote:hover {
            transform: translateX(5px);
        }

        blockquote::before {
            content: '"';
            position: absolute;
            top: 10px;
            left: 10px;
            font-size: 4em;
            color: rgba(139, 69, 19, 0.15);
            font-family: 'Playfair Display', serif;
        }

        .references {
            margin-top: 120px;
            padding: 60px;
            background: var(--card-bg);
            border-radius: 20px;
            box-shadow: var(--shadow-md);
            transition: all 0.3s ease;
        }

        .references h3 {
            font-family: 'Playfair Display', serif;
            font-size: 2em;
            margin-bottom: 40px;
            color: var(--accent-primary);
        }

        .references ol {
            margin-left: 0;
            padding-left: 25px;
        }

        .references li {
            margin: 25px 0;
            line-height: 1.7;
            color: var(--text-secondary);
            transition: transform 0.3s ease;
        }

        .references li:hover {
            transform: translateX(5px);
        }

        .references a {
            color: var(--accent-primary);
            text-decoration: none;
            border-bottom: 2px solid rgba(139, 69, 19, 0.2);
            transition: all 0.3s ease;
            padding-bottom: 2px;
        }

        .references a:hover {
            color: var(--accent-secondary);
            border-bottom-color: var(--accent-secondary);
        }

        @media (max-width: 768px) {
            .header h1 {
                font-size: 2.5em;
            }

            .container {
                padding: 60px 30px;
            }

            h2 {
                font-size: 2em;
            }

            body {
                font-size: 16px;
            }

            .experiment-card {
                padding: 30px;
            }

            .back-to-top {
                bottom: 20px;
                right: 20px;
                width: 50px;
                height: 50px;
                font-size: 20px;
            }

            .theme-toggle {
                top: 70px;
            }
        }

        * {
            transition: background-color 0.3s ease, color 0.3s ease, border-color 0.3s ease;
        }

        ::-webkit-scrollbar {
            width: 10px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-light);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--accent-primary);
            border-radius: 5px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--accent-secondary);
        }
    </style>
</head>
<body>
    <div class="floating-shapes">
        <div class="shape shape1"></div>
        <div class="shape shape2"></div>
        <div class="shape shape3"></div>
    </div>

    <button class="theme-toggle" id="themeToggle" title="Toggle dark mode">üåô</button>
    <div class="scroll-progress" id="progress"></div>

    <div class="header">
        <h1>Optimizer Showdown:<br>Theory Meets Practice</h1>
        <p class="subtitle">A Hands-On Exploration of Optimization Algorithms in Deep Learning</p>
        <p class="meta">By Chris Jalaline Mugot ‚Ä¢ November 22, 2025 ¬∑ 20 minute read</p>
        <div class="tag-container">
            <span class="tag">Deep Learning</span>
            <span class="tag">Optimization</span>
            <span class="tag">PyTorch</span>
            <span class="tag">Experimentation</span>
            <span class="tag">MNIST</span>
        </div>
    </div>

    <div class="container">
        <p class="lead">
            When we train neural networks, we're essentially solving an optimization problem: finding the configuration of weights that minimizes our loss function. But how we navigate this optimization landscape matters tremendously. Different optimization algorithms can mean the difference between a model that converges in hours versus days, or between one that generalizes well versus overfitting. In this post, I'll explore five popular optimizers through both theory and hands-on experimentation with a CNN on MNIST.
        </p>

        <div class="content-section">
            <h2>The Optimization Landscape</h2>
            <p>
                Imagine standing on a mountainous terrain shrouded in fog. Your goal is to find the lowest valley, but you can only see a few feet around you. This is essentially what optimization algorithms do‚Äîthey navigate the high-dimensional loss landscape of neural networks using only local gradient information.
            </p>
            <p>
                The challenge is formidable. Modern neural networks often have millions or billions of parameters, creating a loss landscape that exists in unimaginably high-dimensional space. This landscape is non-convex, meaning it contains countless local minima where an optimizer might get stuck. It contains saddle points that can trap optimization algorithms, and vast plateaus where gradients are nearly zero, making it difficult to determine which direction to move.
            </p>

            <div class="highlight">
                <strong>Key Question:</strong> Given these challenges, how do different optimization algorithms navigate this complex terrain? Which ones are faster? Which ones find better solutions? Let's find out through experimentation.
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>The Contenders</h2>
            <p>
                For this experiment, I selected five popular optimizers that represent different philosophies in optimization:
            </p>

            <div class="experiment-card">
                <h3>1. Stochastic Gradient Descent (SGD)</h3>
                <div class="formula-block">
Œ∏(t+1) = Œ∏(t) - Œ∑ ¬∑ g(t)

Where:
  Œ∏ = model parameters
  Œ∑ = learning rate (0.01 in our experiment)
  g(t) = gradient estimate from mini-batch
                </div>
                <p>
                    <strong>Philosophy:</strong> The simplest approach‚Äîtake steps in the direction opposite to the gradient. SGD uses mini-batches to approximate the true gradient, introducing beneficial noise that can help escape sharp local minima.
                </p>
                <p>
                    <strong>Strengths:</strong> Simple, well-understood, often finds solutions that generalize well to new data.
                </p>
                <p>
                    <strong>Weaknesses:</strong> Can be slow to converge, sensitive to learning rate, struggles with ravines and plateaus.
                </p>
            </div>

            <div class="experiment-card">
                <h3>2. SGD with Momentum</h3>
                <div class="formula-block">
v(t) = Œ≤ ¬∑ v(t-1) + Œ∑ ¬∑ g(t)
Œ∏(t+1) = Œ∏(t) - v(t)

Where:
  v(t) = velocity vector (accumulated gradient)
  Œ≤ = momentum coefficient (0.9 in our experiment)
                </div>
                <p>
                    <strong>Philosophy:</strong> Maintain a running average of past gradients (velocity). If gradients consistently point in one direction, build up speed. If they oscillate, the oscillations cancel out.
                </p>
                <p>
                    <strong>Strengths:</strong> Accelerates progress in consistent directions, dampens oscillations, helps escape plateaus.
                </p>
                <p>
                    <strong>Weaknesses:</strong> Introduces an additional hyperparameter (momentum coefficient), can overshoot minima.
                </p>
            </div>

            <div class="experiment-card">
                <h3>3. RMSProp</h3>
                <div class="formula-block">
E[g¬≤](t) = Œ≥ ¬∑ E[g¬≤](t-1) + (1 - Œ≥) ¬∑ g(t)¬≤
Œ∏(t+1) = Œ∏(t) - (Œ∑ / ‚àö(E[g¬≤](t) + Œµ)) ¬∑ g(t)

Where:
  E[g¬≤](t) = exponentially weighted average of squared gradients
  Œ≥ = decay rate (0.9 in PyTorch default)
  Œµ = small constant for numerical stability (10^-8)
                </div>
                <p>
                    <strong>Philosophy:</strong> Adapt the learning rate for each parameter based on recent gradient magnitudes. Parameters with large gradients get smaller learning rates; parameters with small gradients maintain larger learning rates.
                </p>
                <p>
                    <strong>Strengths:</strong> Works well for non-stationary problems, handles different scales across parameters, popular for RNNs.
                </p>
                <p>
                    <strong>Weaknesses:</strong> Can be sensitive to initial learning rate, may converge prematurely in some cases.
                </p>
            </div>

            <div class="experiment-card">
                <h3>4. Adam (Adaptive Moment Estimation)</h3>
                <div class="formula-block">
m(t) = Œ≤‚ÇÅ ¬∑ m(t-1) + (1 - Œ≤‚ÇÅ) ¬∑ g(t)
v(t) = Œ≤‚ÇÇ ¬∑ v(t-1) + (1 - Œ≤‚ÇÇ) ¬∑ g(t)¬≤
mÃÇ(t) = m(t) / (1 - Œ≤‚ÇÅ^t)
vÃÇ(t) = v(t) / (1 - Œ≤‚ÇÇ^t)
Œ∏(t+1) = Œ∏(t) - Œ∑ ¬∑ mÃÇ(t) / (‚àö(vÃÇ(t)) + Œµ)

Where:
  m(t) = first moment estimate (like momentum)
  v(t) = second moment estimate (like RMSProp)
  Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999 (typical values)
                </div>
                <p>
                    <strong>Philosophy:</strong> Combine the best of momentum and RMSProp. Maintain running averages of both gradients and squared gradients, with bias correction for early training steps.
                </p>
                <p>
                    <strong>Strengths:</strong> Works well out-of-the-box with minimal tuning, fast convergence, handles sparse gradients well.
                </p>
                <p>
                    <strong>Weaknesses:</strong> Can sometimes find solutions that don't generalize as well as SGD, particularly in computer vision.
                </p>
            </div>

            <div class="experiment-card">
                <h3>5. AdamW</h3>
                <div class="formula-block">
Œ∏(t+1) = Œ∏(t) - Œ∑ ¬∑ (mÃÇ(t) / (‚àö(vÃÇ(t)) + Œµ) + Œª ¬∑ Œ∏(t))

Where:
  Œª = weight decay coefficient
  Weight decay applied directly to parameters, not through gradient
                </div>
                <p>
                    <strong>Philosophy:</strong> Fix Adam's incorrect implementation of weight decay by decoupling it from the gradient-based update. Apply weight decay directly to parameters after the adaptive update.
                </p>
                <p>
                    <strong>Strengths:</strong> Better generalization than vanilla Adam, maintains Adam's fast convergence, increasingly popular for transformers.
                </p>
                <p>
                    <strong>Weaknesses:</strong> Still inherits some of Adam's limitations compared to SGD with momentum.
                </p>
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>The Experiment</h2>
            <h3>Experimental Setup</h3>
            <p>
                To fairly compare these optimizers, I designed a controlled experiment using the MNIST handwritten digit classification dataset‚Äîa classic benchmark in computer vision. Here's the setup:
            </p>

            <div class="experiment-card">
                <h3>Model Architecture</h3>
                <div class="code-block">
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),  # 28x28x1 -> 28x28x32
            nn.ReLU(),
            nn.MaxPool2d(2),                 # 28x28x32 -> 14x14x32
            nn.Conv2d(32, 64, 3, padding=1), # 14x14x32 -> 14x14x64
            nn.ReLU(),
            nn.MaxPool2d(2)                  # 14x14x64 -> 7x7x64
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(7*7*64, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)
        return x
                </div>
                <p>
                    A simple but effective CNN with two convolutional layers followed by two fully connected layers. Total parameters: approximately 101,770‚Äîsmall enough to train quickly but large enough to demonstrate optimizer differences.
                </p>
            </div>

            <div class="experiment-card">
                <h3>Training Configuration</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>Dataset:</strong> MNIST (60,000 training images, 10,000 test images)</li>
                    <li><strong>Batch Size:</strong> 64</li>
                    <li><strong>Epochs:</strong> 5</li>
                    <li><strong>Loss Function:</strong> Cross-Entropy Loss</li>
                    <li><strong>Learning Rates:</strong> 0.01 for SGD variants, 0.001 for adaptive optimizers</li>
                    <li><strong>Hardware:</strong> CPU (for reproducibility)</li>
                </ul>
            </div>

            <div class="experiment-card">
                <h3>The Code</h3>
                <div class="code-block">
# Training loop for each optimizer
optimizers = {
    "SGD": lambda params: optim.SGD(params, lr=0.01),
    "SGD_Momentum": lambda params: optim.SGD(params, lr=0.01, momentum=0.9),
    "RMSProp": lambda params: optim.RMSprop(params, lr=0.001),
    "Adam": lambda params: optim.Adam(params, lr=0.001),
    "AdamW": lambda params: optim.AdamW(params, lr=0.001)
}

for opt_name, opt_func in optimizers.items():
    model = SimpleCNN()
    optimizer = opt_func(model.parameters())
    
    for epoch in range(1, EPOCHS + 1):
        loss = train_one_epoch(model, optimizer, criterion, train_loader)
        acc = test_accuracy(model, test_loader)
        # Record results...
                </div>
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>Results: What the Data Tells Us</h2>
            <p>
                After training each optimizer for 5 epochs, the results revealed clear patterns about how different optimization algorithms behave in practice. Let's dive into the data.
            </p>

            <div class="experiment-card">
                <h3>Final Performance (Epoch 5)</h3>
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Optimizer</th>
                            <th>Final Loss</th>
                            <th>Final Accuracy</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>SGD</strong></td>
                            <td>0.0652</td>
                            <td>98.13%</td>
                            <td>+5.90%</td>
                        </tr>
                        <tr>
                            <td><strong>SGD + Momentum</strong></td>
                            <td>0.0185</td>
                            <td>98.87%</td>
                            <td>+0.55%</td>
                        </tr>
                        <tr>
                            <td><strong>RMSProp</strong></td>
                            <td>0.0158</td>
                            <td>98.87%</td>
                            <td>+1.08%</td>
                        </tr>
                        <tr>
                            <td><strong>Adam</strong></td>
                            <td>0.0153</td>
                            <td>98.96%</td>
                            <td>+0.49%</td>
                        </tr>
                        <tr>
                            <td><strong>AdamW</strong></td>
                            <td>0.0165</td>
                            <td>99.12%</td>
                            <td>+0.64%</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Epoch-by-Epoch Analysis</h3>

            <div class="experiment-card">
                <h3>Epoch 1: The Starting Sprint</h3>
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Optimizer</th>
                            <th>Loss</th>
                            <th>Accuracy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>SGD</td>
                            <td>0.5205</td>
                            <td>92.33%</td>
                        </tr>
                        <tr>
                            <td>SGD + Momentum</td>
                            <td>0.1827</td>
                            <td>98.32%</td>
                        </tr>
                        <tr>
                            <td>RMSProp</td>
                            <td>0.1303</td>
                            <td>97.79%</td>
                        </tr>
                        <tr>
                            <td>Adam</td>
                            <td>0.1338</td>
                            <td>98.47%</td>
                        </tr>
                        <tr>
                            <td>AdamW</td>
                            <td>0.1335</td>
                            <td>98.48%</td>
                        </tr>
                    </tbody>
                </table>
                <p>
                    <strong>Key Observation:</strong> Plain SGD struggles significantly in the first epoch (92.33% accuracy), while all other optimizers achieve 97%+ accuracy. This demonstrates the power of adaptive learning rates and momentum‚Äîthey allow the model to make much more progress with the same number of parameter updates.
                </p>
            </div>

            <div class="experiment-card">
                <h3>Convergence Speed</h3>
                <p>
                    Looking at how quickly each optimizer reaches high accuracy reveals interesting patterns:
                </p>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>Fastest to 98%:</strong> Adam and AdamW (Epoch 1)</li>
                    <li><strong>Fastest to 99%:</strong> SGD with Momentum (Epoch 4)</li>
                    <li><strong>Slowest Overall:</strong> Plain SGD (needed all 5 epochs to reach 98.13%)</li>
                </ul>
                <p>
                    The adaptive optimizers (Adam, AdamW, RMSProp) achieve strong performance immediately, while SGD requires patience. However, SGD with momentum strikes an excellent balance, reaching 98.32% in just the first epoch.
                </p>
            </div>

            <div class="experiment-card">
                <h3>Loss Trajectory Analysis</h3>
                <div class="chart-container">
                    <p><strong>Loss Reduction per Optimizer:</strong></p>
                    <ul style="margin-left: 20px; line-height: 2;">
                        <li><strong>SGD:</strong> 0.5205 ‚Üí 0.0652 (87.5% reduction)</li>
                        <li><strong>SGD + Momentum:</strong> 0.1827 ‚Üí 0.0185 (89.9% reduction)</li>
                        <li><strong>RMSProp:</strong> 0.1303 ‚Üí 0.0158 (87.9% reduction)</li>
                        <li><strong>Adam:</strong> 0.1338 ‚Üí 0.0153 (88.6% reduction)</li>
                        <li><strong>AdamW:</strong> 0.1335 ‚Üí 0.0165 (87.6% reduction)</li>
                    </ul>
                </div>
                <p>
                    All optimizers successfully reduce loss, but their paths differ significantly. SGD starts with high loss and gradually improves, while adaptive methods begin with much lower loss and make smaller, steadier improvements.
                </p>
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>Key Findings and Insights</h2>

            <div class="highlight">
                <h3>1. The Momentum Effect is Powerful</h3>
                <p>
                    Adding momentum to SGD transformed it from the worst performer (92.33% ‚Üí 98.13%) to one of the best (98.32% ‚Üí 98.87%). The momentum coefficient of 0.9 helped the optimizer build velocity in consistent directions and dampen oscillations. This 6% accuracy boost in the first epoch alone demonstrates why momentum has become a standard component of modern optimization.
                </p>
            </div>

            <div class="highlight">
                <h3>2. Adaptive Methods Excel at Early Training</h3>
                <p>
                    RMSProp, Adam, and AdamW all achieved 97%+ accuracy in the first epoch, while plain SGD only reached 92%. The adaptive learning rates allow these optimizers to make appropriate step sizes for each parameter automatically, eliminating much of the manual tuning required for SGD. This is why Adam has become the default choice for many practitioners‚Äîit "just works" with minimal configuration.
                </p>
            </div>

            <div class="highlight">
                <h3>3. AdamW's Slight Edge in Generalization</h3>
                <p>
                    AdamW achieved the highest final accuracy (99.12%), marginally outperforming Adam (98.96%). While the difference seems small, it's consistent with research showing that AdamW's proper weight decay implementation leads to better generalization. For this relatively simple task, the difference is modest, but on larger models and datasets, this gap often widens.
                </p>
            </div>

            <div class="highlight">
                <h3>4. Training Stability Varies</h3>
                <p>
                    Looking at epoch-to-epoch changes, SGD with momentum showed some instability (dropping from 99.14% in Epoch 4 to 98.87% in Epoch 5), while the adaptive methods maintained more stable accuracy curves. This is both a strength and weakness‚Äîthe noise in SGD can help it escape poor local minima, but it also means less predictable training dynamics.
                </p>
            </div>

            <div class="highlight">
                <h3>5. The Learning Rate Matters Greatly</h3>
                <p>
                    I used lr=0.01 for SGD variants and lr=0.001 for adaptive methods. These are typical values, but they're not universal. Plain SGD's poor performance might improve with a different learning rate schedule (e.g., starting higher and decaying). This highlights a key trade-off: adaptive methods are more robust to learning rate choice, while SGD requires more careful tuning but can achieve excellent results when properly configured.
                </p>
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>Practical Recommendations</h2>
            <p>
                Based on this experiment and broader deep learning practice, here's my guidance for choosing an optimizer:
            </p>

            <div class="experiment-card">
                <h3>Start with Adam or AdamW</h3>
                <p>
                    For most projects, especially when you're prototyping or working with unfamiliar architectures, <strong>Adam or AdamW</strong> is the best starting point. These optimizers:
                </p>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li>Work well with default hyperparameters (lr=0.001, Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999)</li>
                    <li>Converge quickly, saving development time</li>
                    <li>Handle different scales across parameters automatically</li>
                    <li>Require minimal tuning to get reasonable results</li>
                </ul>
                <p>
                    Use <strong>AdamW</strong> over Adam when training large models or when regularization is important, as its proper weight decay implementation typically yields better generalization.
                </p>
            </div>

            <div class="experiment-card">
                <h3>Consider SGD with Momentum for Production</h3>
                <p>
                    Once you've validated your architecture and are ready for final training runs, <strong>SGD with momentum</strong> deserves serious consideration, especially for computer vision tasks. While it requires more hyperparameter tuning (learning rate, momentum coefficient, learning rate schedule), it often achieves:
                </p>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li>Better generalization to unseen data</li>
                    <li>More robust final models</li>
                    <li>State-of-the-art results in image classification benchmarks</li>
                </ul>
                <p>
                    The trade-off is development time‚Äîyou'll need to experiment with learning rates and schedules‚Äîbut the payoff in model quality can be worth it.
                </p>
            </div>

            <div class="experiment-card">
                <h3>RMSProp for Recurrent Networks</h3>
                <p>
                    <strong>RMSProp</strong> remains popular for training RNNs and LSTMs, where the dynamic range of gradients can vary dramatically across time steps. Its ability to adapt quickly to changing gradient statistics makes it well-suited to sequential data.
                </p>
            </div>

            <div class="experiment-card">
                <h3>Don't Forget Learning Rate Schedules</h3>
                <p>
                    Regardless of which optimizer you choose, <strong>learning rate scheduling</strong> often provides significant benefits. Common strategies include:
                </p>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>Step Decay:</strong> Reduce learning rate by a factor every N epochs</li>
                    <li><strong>Cosine Annealing:</strong> Smoothly decay learning rate following a cosine curve</li>
                    <li><strong>Reduce on Plateau:</strong> Decrease learning rate when validation loss stops improving</li>
                    <li><strong>Warmup:</strong> Gradually increase learning rate at the start of training</li>
                </ul>
                <p>
                    In my experiment, I used fixed learning rates to isolate the optimizer's effect, but in practice, adding a schedule often improves results for all optimizers.
                </p>
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>Beyond This Experiment</h2>
            <p>
                While this experiment provides valuable insights, it's important to acknowledge its limitations and consider what we might explore next:
            </p>

            <div class="experiment-card">
                <h3>Limitations of This Study</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>Dataset Simplicity:</strong> MNIST is relatively easy‚Äîdifferences between optimizers become more pronounced on complex datasets like ImageNet or large language modeling tasks</li>
                    <li><strong>Short Training:</strong> Only 5 epochs‚Äîlonger training runs might reveal different patterns, especially regarding overfitting and generalization</li>
                    <li><strong>Fixed Learning Rates:</strong> Using learning rate schedules would likely change the relative performance</li>
                    <li><strong>Single Architecture:</strong> Results might differ for ResNets, Transformers, or other architectures</li>
                    <li><strong>No Hyperparameter Tuning:</strong> I used common default values, but optimal settings differ per optimizer and task</li>
                </ul>
            </div>

            <div class="experiment-card">
                <h3>Future Experiments to Consider</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>Larger Scale:</strong> Test on CIFAR-10/100 or a subset of ImageNet</li>
                    <li><strong>Different Architectures:</strong> Compare optimizers across MLPs, ResNets, and Transformers</li>
                    <li><strong>Learning Rate Sensitivity:</strong> Grid search over learning rates for each optimizer</li>
                    <li><strong>Longer Training:</strong> Train for 50+ epochs to observe long-term behavior</li>
                    <li><strong>Regularization Effects:</strong> Add dropout, data augmentation, label smoothing</li>
                    <li><strong>Advanced Optimizers:</strong> Test LAMB, Lookahead, Ranger, or learned optimizers</li>
                    <li><strong>Generalization Gap:</strong> Measure train vs test performance carefully</li>
                </ul>
            </div>
        </div>

        <div class="divider"></div>

        <div class="content-section">
            <h2>Conclusion: Choosing Wisely</h2>
            <p>
                Optimization algorithms are the unsung heroes of deep learning. While we often focus on architectures, datasets, and applications, the optimizer is what actually makes learning happen‚Äîit's the engine that transforms random weights into useful representations.
            </p>
            <p>
                Through this experiment, we've seen that different optimizers offer different trade-offs. Adam and AdamW provide fast, reliable convergence with minimal tuning‚Äîperfect for research and prototyping. SGD with momentum requires more care but can achieve superior generalization‚Äîideal for production models where that extra accuracy matters. RMSProp excels at handling non-stationary problems like recurrent networks.
            </p>

            <blockquote>
                The best optimizer is not a universal constant‚Äîit depends on your task, your timeline, and your priorities. But understanding how different optimizers behave gives you the knowledge to make informed choices rather than relying on defaults.
            </blockquote>

            <p>
                What surprised me most in this experiment was just how much momentum mattered. The gap between plain SGD (92.33% first epoch accuracy) and SGD with momentum (98.32%) was dramatic‚Äîa single hyperparameter made a 6% difference. This reinforces an important lesson: simple modifications to optimization, when properly applied, can have outsized impacts on performance.
            </p>
            <p>
                As neural networks continue to grow in scale and complexity, optimization algorithms evolve alongside them. New techniques like LAMB enable training with massive batch sizes. Learned optimizers use neural networks to learn optimization strategies. Second-order methods are being revisited with modern approximations. The field is far from solved, and there's still much to discover about how to efficiently navigate these high-dimensional loss landscapes.
            </p>
            <p>
                For practitioners, the key takeaway is pragmatic: start with Adam or AdamW for fast iteration, consider SGD with momentum when you need the absolute best generalization, and always be prepared to experiment. The "best" optimizer for your specific problem is ultimately an empirical question, but armed with an understanding of how these algorithms work, you're well-equipped to make that determination.
            </p>
            <p>
                The code for this experiment is straightforward enough to run in minutes on a laptop, yet it reveals principles that scale to billion-parameter models. I encourage you to replicate it, modify it, and extend it. Try different learning rates. Add learning rate schedules. Test on different datasets. The best way to understand optimization is not just to read about it, but to experience how these algorithms behave under your own observation.
            </p>
            <p>
                Happy optimizing! üöÄ
            </p>
        </div>

        <div class="divider"></div>

        <div class="references">
            <h3>References & Further Reading</h3>
            <ol>
                <li>Ruder, S. (2016). "An overview of gradient descent optimization algorithms." <em>arXiv preprint arXiv:1609.04747</em>. <a href="https://arxiv.org/abs/1609.04747" target="_blank">https://arxiv.org/abs/1609.04747</a></li>
                <li>Kingma, D. P., & Ba, J. (2014). "Adam: A method for stochastic optimization." <em>arXiv preprint arXiv:1412.6980</em>. <a href="https://arxiv.org/abs/1412.6980" target="_blank">https://arxiv.org/abs/1412.6980</a></li>
                <li>Loshchilov, I., & Hutter, F. (2017). "Decoupled weight decay regularization." <em>arXiv preprint arXiv:1711.05101</em>. <a href="https://arxiv.org/abs/1711.05101" target="_blank">https://arxiv.org/abs/1711.05101</a></li>
                <li>Bottou, L., Curtis, F. E., & Nocedal, J. (2018). "Optimization methods for large-scale machine learning." <em>SIAM Review, 60</em>(2), 223-311.</li>
                <li>Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., & Tang, P. T. P. (2016). "On large-batch training for deep learning: Generalization gap and sharp minima." <em>arXiv preprint arXiv:1609.04836</em>.</li>
                <li>PyTorch Documentation. "torch.optim - Optimization Algorithms." <a href="https://pytorch.org/docs/stable/optim.html" target="_blank">https://pytorch.org/docs/stable/optim.html</a></li>
                <li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press. Chapter 8: Optimization for Training Deep Models.</li>
                <li>Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., & Recht, B. (2017). "The marginal value of adaptive gradient methods in machine learning." <em>Advances in Neural Information Processing Systems</em>, 30.</li>
            </ol>

            <div style="margin-top: 40px; padding-top: 40px; border-top: 2px solid var(--border-color);">
                <h3 style="margin-bottom: 20px;">About This Post</h3>
                <p>
                    This blog post was created as part of a deep learning portfolio project. The experiment was conducted using PyTorch 2.0+ on MNIST, with all code available for reproduction. If you found this analysis helpful or have suggestions for future experiments, feel free to reach out!
                </p>
                <p style="margin-top: 20px;">
                    <strong>Repository:</strong> <a href="https://github.com/yourusername/optimizer-experiments" target="_blank">github.com/yourusername/optimizer-experiments</a><br>
                    <strong>Contact:</strong> chris.mugot@example.com<br>
                    <strong>Date:</strong> November 22, 2025
                </p>
            </div>
        </div>
    </div>

    <button class="back-to-top" id="backToTop">‚Üë</button>

    <script>
    // Prevent multiple initializations in Jupyter
    (function initializeBlog() {
        'use strict';
        
        // Check if already initialized
        if (window.blogOptimizersInitialized) {
            console.log('Blog already initialized, skipping...');
            return;
        }
        window.blogOptimizersInitialized = true;

        // Theme toggle
        const themeToggle = document.getElementById('themeToggle');
        const body = document.body;
        
        // Check for saved theme preference
        try {
            const currentTheme = localStorage.getItem('theme') || 'light';
            if (currentTheme === 'dark') {
                body.classList.add('dark-mode');
                themeToggle.textContent = '‚òÄÔ∏è';
            }
        } catch (e) {
            // LocalStorage might not be available in some Jupyter environments
            console.log('LocalStorage not available, using default theme');
        }

        themeToggle.addEventListener('click', function() {
            body.classList.toggle('dark-mode');
            const theme = body.classList.contains('dark-mode') ? 'dark' : 'light';
            themeToggle.textContent = theme === 'dark' ? '‚òÄÔ∏è' : 'üåô';
            
            try {
                localStorage.setItem('theme', theme);
            } catch (e) {
                // Silently fail if localStorage isn't available
            }
        });

        // Scroll progress bar
        function updateScrollProgress() {
            const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const scrolled = (winScroll / height) * 100;
            document.getElementById('progress').style.width = scrolled + '%';

            // Back to top button visibility
            const backToTop = document.getElementById('backToTop');
            if (winScroll > 300) {
                backToTop.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
            }
        }

        window.addEventListener('scroll', updateScrollProgress);

        // Back to top functionality
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Smooth scroll for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Initialize cards as visible (skip IntersectionObserver in Jupyter)
        document.querySelectorAll('.experiment-card, .highlight').forEach(card => {
            card.style.opacity = '1';
            card.style.transform = 'translateY(0)';
        });

        console.log('Blog initialized successfully!');
    })();
    </script>
</body>
</html>