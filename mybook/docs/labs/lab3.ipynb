{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3e6e6e",
   "metadata": {},
   "source": [
    " ##  Understanding Deep Learning : Lab 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412c34f",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation in Python <br>\n",
    "In this activity, we extend the feedforward neural network by adding **backpropagation**. \n",
    "We first perform a forward pass to compute the weighted sums, apply the **ReLU activation function**, and generate the prediction. \n",
    "Then, we calculate the error using **Mean Squared Error (MSE)** and apply **backpropagation** to compute gradients with respect to the weights and biases. \n",
    "Finally, we update the parameters using **gradient descent** with the specified learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a374c3b",
   "metadata": {},
   "source": [
    "Here is the given problem setup:\n",
    "\n",
    "<img src=\"https://i.ibb.co/TM29FCJJ/Screenshot-2025-09-13-at-1-31-33-PM.png\" width=\"1200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407c47e",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e726bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4ddb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Inputs and target (from Task 2)\n",
    "# -----------------------------\n",
    "\n",
    "x = np.array([1, 0, 1])   # input vector\n",
    "y = np.array([1])         # target output\n",
    "lr = 0.001                # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e08eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Initialize weights and biases (from Task 2)\n",
    "# -----------------------------\n",
    "# Hidden layer weights (3 inputs -> 2 hidden neurons)\n",
    "\n",
    "W_hidden = np.array([\n",
    "    [0.2, -0.3],   # weights for input x1\n",
    "    [0.4,  0.1],   # weights for input x2\n",
    "    [-0.5, 0.2]    # weights for input x3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cc98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases for hidden neurons\n",
    "b_hidden = np.array([-0.4, 0.2])\n",
    "\n",
    "# Output layer weights (2 hidden -> 1 output neuron)\n",
    "W_output = np.array([[-0.3], [-0.2]])\n",
    "\n",
    "# Bias for output neuron\n",
    "b_output = np.array([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f256d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Activation functions\n",
    "# -----------------------------\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381c2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Forward pass\n",
    "# -----------------------------\n",
    "\n",
    "# Hidden layer computation\n",
    "Z_hidden = np.dot(x, W_hidden) + b_hidden   # weighted sum\n",
    "H = relu(Z_hidden)                         # apply ReLU\n",
    "\n",
    "# Output layer computation\n",
    "Z_output = np.dot(H, W_output) + b_output\n",
    "y_hat = relu(Z_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b46c18",
   "metadata": {},
   "source": [
    "### FeedForward Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e51586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "Z_hidden = [-0.7  0.1]\n",
      "H (hidden activations) = [0.  0.1]\n",
      "Z_output = [0.08]\n",
      "y_hat (prediction) = [0.08]\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward Pass:\")\n",
    "print(\"Z_hidden =\", Z_hidden)\n",
    "print(\"H (hidden activations) =\", H)\n",
    "print(\"Z_output =\", Z_output)\n",
    "print(\"y_hat (prediction) =\", y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053996fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.8464\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Compute loss (Mean Squared Error)\n",
    "# -----------------------------\n",
    "loss = np.mean((y - y_hat) ** 2)\n",
    "print(\"Loss =\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06371b",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fcd1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backward pass\n",
    "# -----------------------------\n",
    "# Derivative of loss w.r.t y_hat (MSE derivative)\n",
    "dL_dyhat = 2 * (y_hat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff46a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative through ReLU at output\n",
    "dyhat_dZout = relu_derivative(Z_output)\n",
    "dL_dZout = dL_dyhat * dyhat_dZout\n",
    "\n",
    "# Gradients for output weights and bias\n",
    "dL_dWout = H.reshape(-1,1) @ dL_dZout.reshape(1,-1)   # outer product\n",
    "dL_dbout = dL_dZout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212bf41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients for output weights and bias\n",
    "dL_dWout = H.reshape(-1,1) @ dL_dZout.reshape(1,-1)   # outer product\n",
    "dL_dbout = dL_dZout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21c617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop to hidden layer\n",
    "dL_dH = dL_dZout @ W_output.T\n",
    "dH_dZhidden = relu_derivative(Z_hidden)\n",
    "dL_dZhidden = dL_dH * dH_dZhidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833bd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients for hidden weights and biases\n",
    "dL_dWhidden = x.reshape(-1,1) @ dL_dZhidden.reshape(1,-1)\n",
    "dL_dbhidden = dL_dZhidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814342e9",
   "metadata": {},
   "source": [
    "### Backward Propogation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ca9d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Pass:\n",
      "dL_dWout = \n",
      " [[ 0.   ]\n",
      " [-0.184]]\n",
      "dL_dbout = \n",
      " [-1.84]\n",
      "dL_dWhidden = \n",
      " [[0.    0.368]\n",
      " [0.    0.   ]\n",
      " [0.    0.368]]\n",
      "dL_dbhidden = \n",
      " [0.    0.368]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBackward Pass:\")\n",
    "print(\"dL_dWout =\", \"\\n\",dL_dWout)\n",
    "print(\"dL_dbout =\", \"\\n\", dL_dbout)\n",
    "print(\"dL_dWhidden =\",\"\\n\", dL_dWhidden)\n",
    "print(\"dL_dbhidden =\",\"\\n\", dL_dbhidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a5a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Update weights (Gradient Descent)\n",
    "# -----------------------------\n",
    "W_output -= lr * dL_dWout\n",
    "b_output -= lr * dL_dbout\n",
    "W_hidden -= lr * dL_dWhidden\n",
    "b_hidden -= lr * dL_dbhidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bcc77",
   "metadata": {},
   "source": [
    "#### Final Results (Updated Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42554933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Parameters:\n",
      "W_hidden = \n",
      " [[ 0.2      -0.300368]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199632]]\n",
      "b_hidden = \n",
      " [-0.4       0.199632]\n",
      "W_output = \n",
      " [[-0.3     ]\n",
      " [-0.199816]]\n",
      "b_output = \n",
      " [0.10184]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUpdated Parameters:\")\n",
    "print(\"W_hidden =\",\"\\n\", W_hidden)\n",
    "print(\"b_hidden =\",\"\\n\", b_hidden)\n",
    "print(\"W_output =\", \"\\n\",W_output)\n",
    "print(\"b_output =\",\"\\n\", b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73468ab0",
   "metadata": {},
   "source": [
    "### Conclusion / Learnings  \n",
    "\n",
    "<div style=\"border: 2px solid #4CAF50; background-color: #e8f5e9; padding: 15px; border-radius: 10px; margin-top: 10px; margin-bottom: 10px;\">\n",
    "In this activity, we learned how to implement both <strong>forward and backward propagation</strong> in a feedforward neural network.  \n",
    "During the forward pass, we computed the weighted sums of the inputs, applied the <strong>ReLU activation function</strong>, and generated the network’s prediction.  \n",
    "We then calculated the error using <strong>Mean Squared Error (MSE)</strong>, which allowed us to measure how far the prediction was from the target value.  \n",
    "Through backpropagation, we derived the gradients of the loss with respect to weights and biases using the <strong>chain rule</strong>, and applied <strong>gradient descent</strong> to update the parameters.  \n",
    "This exercise demonstrated the complete learning process of a neural network—how it makes predictions, evaluates errors, and adjusts itself to improve performance over time.  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
